# A directory For all the produced outputs
artifacts_root: artifacts

data_ingestion:
  root_dir: artifacts/data_ingestion
  data_folder_path: artifacts/data_ingestion
  AWS_BUCKET_NAME: samyarsworld-nucleidata
  AWS_DATASET_NAME: data.zip

data_validation:
  root_dir: artifacts/data_validation
  data_path: artifacts/data_ingestion/data
  required_files: ["train", "test"]
  status_path: artifacts/data_validation/status.txt

data_construction:
  root_dir: artifacts/data_construction
  train_path: artifacts/data_ingestion/data/train
  h_params:
    TRAIN_SIZE: 0.7
    IMG_WIDTH: 128
    IMG_HEIGHT: 128
    BATCH_SIZE: 32

model_trainer:
  root_dir: artifacts/model_trainers
  IMG_CHANNELS: 3
  MASK_CHANNELS: 1
  # pre_trained_model: google/pegasus-cnn_dailymail
  # pre_trained_model_path: artifacts/model_trainers/UNET/model.pt
  model_path: artifacts/model_trainers/UNET/model.pt

  # Hyper parameters
  h_params:
    epochs: 1
    lr: 0.01
    channels: [16, 32, 64, 128, 256]
    # warmup_steps: 500
    # train_batch_size: 1
    # eval_batch_size: 1
    # weight_decay: 0.01
    # logging_steps: 10
    # evaluation_strategy: "steps"
    # eval_steps: 500
    # save_steps: 1000000
    # gradient_accumulation_steps: 16

model_evaluation:
  root_dir: artifacts/model_evaluation
  data_path: artifacts/data_construction/dataset
  # model_path: artifacts/model_trainers/pegasus-cnn_dailymail/model
  # arguments_path: artifacts/model_trainers/pegasus-cnn_dailymail/arguments
  metric_file_name: artifacts/model_evaluation/metrics.csv
  params:
    eval_batch_size: 2
    n_samples: 10

target_prediction:
  root_dir: artifacts/target_prediction
  # model_path: artifacts/model_trainers/pegasus-cnn_dailymail/model
